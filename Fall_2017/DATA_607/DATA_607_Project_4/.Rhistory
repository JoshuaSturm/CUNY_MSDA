<<<<<<< HEAD
nrow(comb)
# test.ham <- tm_map(test.ham, PlainTextDocument)
# test.spam <- tm_map(test.spam, PlainTextDocument)
comb <- c(test.ham, test.spam)
tr <- comb[1:4393,]
# test.ham <- tm_map(test.ham, PlainTextDocument)
# test.spam <- tm_map(test.spam, PlainTextDocument)
comb <- c(test.ham, test.spam)
tr <- comb[1:4393, ]
tr <- comb[1:4393, ]
comb
comb <- TermDocumentMatrix(comb)
dim(comb)
tr <- comb[1:4393, ]
comb
comb <- comb %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument)
# direc <- "train/ham/"
# train.ham <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
# direc <- "train/spam/"
# train.spam <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
direc <- "test/ham/"
test.ham <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
direc <- "test/spam/"
test.spam <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
meta(test.ham)
meta(test.spam)
meta(train.ham)
head(meta(test.ham))
print(meta(test.spam))
View(meta(test.ham))
comb <- c(test.ham, test.spam)
comb <- c(test.ham, test.spam, recursive=T)
comb <- TermDocumentMatrix(comb)
comb
tr <- comb[1:4393, ]
ts <- comb[4394:5857, ]
ts$type
ts$nrow
ts$dimnames$Terms
ts$dimnames$Docs
comb <- removeSparseTerms(comb, 0.95)
tr <- comb[1:4393, ]
comb
comb <- TermDocumentMatrix(comb)
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
# Set working directory, import files as a corpus, and then convert each to a document matrix
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
# train.spam.corpus <- VCorpus(DirSource(directory="train/spam"))
# train.ham.corpus <- TermDocumentMatrix(VCorpus(DirSource(directory="train/ham")))
# test.spam.corpus <- TermDocumentMatrix(Corpus(DirSource(directory="test/spam")))
# test.ham.corpus <- TermDocumentMatrix(Corpus(DirSource(directory="test/ham")))
# direc <- "train/ham/"
# train.ham <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
# direc <- "train/spam/"
# train.spam <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
direc <- "test/ham/"
test.ham <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
direc <- "test/spam/"
test.spam <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
test.ham <- test.ham %>%
tm_map(content_transformer(tolower)) %>%
=======
>>>>>>> 1be872a834670d7cf7e4e902e950577bdddf79aa
tm_map(stripWhitespace) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument) %>%
removeSparseTerms(sparse = 0.95)
test.ham <- test.ham %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument)
test.spam <- test.spam %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument)
# train.ham <- train.ham %>%
#     tm_map(content_transformer(tolower)) %>%
#     tm_map(stripWhitespace) %>%
#     tm_map(removePunctuation) %>%
#     tm_map(removeNumbers) %>%
#     tm_map(removeWords, stopwords("english")) %>%
#     tm_map(stemDocument)
#
# train.spam <- train.spam %>%
#     tm_map(content_transformer(tolower)) %>%
#     tm_map(stripWhitespace) %>%
#     tm_map(removePunctuation) %>%
#     tm_map(removeNumbers) %>%
#     tm_map(removeWords, stopwords("english")) %>%
#     tm_map(stemDocument)
comb <- c(test.ham, test.spam, recursive=T)
comb <- TermDocumentMatrix(comb)
comb <- removeSparseTerms(0.95)
comb <- removeSparseTerms(sparse = 0.95)
comb <- removeSparseTerms(comb, sparse = 0.95)
tr <- comb[1:4393, ]
tr <- sample(comb)
comb
container <- create_container(comb, trainSize = 1:4393, testSize = 4394:5857, virgin = FALSE)
dim(comb)
container <- create_container(comb, trainSize = 1:248, testSize = 249:326, virgin = FALSE)
meta(comb)
comb$nrow
comb$dimnames
comb$dimnames$Ters
comb$dimnames$Terms
test.ham
dim(test.ham)
test.ham$`0001.1999-12-10.kaminski.ham.txt`
test.ham[[1]]
test.ham[[1]]$content
test.ham[[1]]$meta
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
# Set working directory, import files as a corpus, and then convert each to a document matrix
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
# train.spam.corpus <- VCorpus(DirSource(directory="train/spam"))
# train.ham.corpus <- TermDocumentMatrix(VCorpus(DirSource(directory="train/ham")))
# test.spam.corpus <- TermDocumentMatrix(Corpus(DirSource(directory="test/spam")))
# test.ham.corpus <- TermDocumentMatrix(Corpus(DirSource(directory="test/ham")))
# direc <- "train/ham/"
# train.ham <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
# direc <- "train/spam/"
# train.spam <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
direc <- "test/ham/"
test.ham <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
meta(test.ham, tag = "type") <- "ham"
direc <- "test/spam/"
test.spam <- VCorpus(DirSource(directory = direc), readerControl = list(language = "en"))
meta(test.spam, tag = "type") <- "spam"
test.ham <- test.ham %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument)
test.spam <- test.spam %>%
tm_map(content_transformer(tolower)) %>%
tm_map(stripWhitespace) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument)
# train.ham <- train.ham %>%
#     tm_map(content_transformer(tolower)) %>%
#     tm_map(stripWhitespace) %>%
#     tm_map(removePunctuation) %>%
#     tm_map(removeNumbers) %>%
#     tm_map(removeWords, stopwords("english")) %>%
#     tm_map(stemDocument)
#
# train.spam <- train.spam %>%
#     tm_map(content_transformer(tolower)) %>%
#     tm_map(stripWhitespace) %>%
#     tm_map(removePunctuation) %>%
#     tm_map(removeNumbers) %>%
#     tm_map(removeWords, stopwords("english")) %>%
#     tm_map(stemDocument)
# test.ham <- tm_map(test.ham, PlainTextDocument)
# test.spam <- tm_map(test.spam, PlainTextDocument)
comb <- c(test.ham, test.spam, recursive=T)
comb <- TermDocumentMatrix(comb)
comb <- removeSparseTerms(comb, sparse = 0.95)
tr <- sample(comb)
# tr <- comb[1:4393, ]
# ts <- comb[4394:5857, ]
# test.ham <- TermDocumentMatrix(test.ham)
# test.spam <- TermDocumentMatrix(test.spam)
#
# test.ham <- removeSparseTerms(test.ham, 0.95)
# test.spam <- removeSparseTerms(test.spam, 0.95)
types <- unlist(meta(tr, "type")[,1])
types <- unlist(meta(tr, "type")[,1])
types <- unlist(meta(tr, "type")[,1])
install.packages(c("lubridate", "openssl", "rlang"))
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(stringr)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
# Training Files
ham.one.dir <- list.files("easy_ham/", full.names = T, recursive = F)
spam.one.dir <- list.files("spam/", full.names = T, recursive = F)
# Testing Files
ham.two.dir <- list.files("easy_ham_2/", full.names = T, recursive = F)
spam.two.dir <- list.files("spam_2/", full.names = T, recursive = F)
# Combine testing files
comb <- c(ham.two.dir, spam.two.dir)
head(readLines(ham.one.dir[1]), 10)
tail(readLines(ham.one.dir[1]), 10)
get.msg <- function(path) {
con <- file(path, open="rt")
text <- readLines(con)
msg <- text[seq(which(text == "")[1] + 1, length(text))]
close(con)
return(paste(msg, collapse = "\n"))
}
spam.path <- "spam/"
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which( !str_detect(spam.docs,"^0000.") & spam.docs != 'cmds' )]
all.spam <- sapply(spam.docs, function(p)get.msg(file.path(spam.path, p, sep = "")))
ham.path <- "easy_ham/"
ham.docs <- dir(ham.path)
ham.docs <- ham.docs[which(ham.docs!="cmds")]
all.ham <- sapply(ham.docs, function(p)get.msg(paste(ham.path,p,sep="")))
spam.2.path <- "spam_2/"
spam.2.docs <- dir(spam.path)
spam.2.docs <- spam.docs[which( !str_detect(spam.docs,"^0000.") & spam.docs != 'cmds' )]
all.spam.2 <- sapply(spam.docs, function(p)get.msg(file.path(spam.path, p, sep = "")))
ham.2.path <- "easy_ham_2/"
ham.2.docs <- dir(ham.path)
ham.2.docs <- ham.docs[which(ham.docs!="cmds")]
all.ham.2 <- sapply(ham.docs, function(p)get.msg(paste(ham.path,p,sep="")))
spam.corpus <- Corpus(VectorSource(all.spam))
meta(spam.corpus, tag = "class") <- "spam"
ham.corpus <- Corpus(VectorSource(all.ham))
meta(ham.corpus, tag = "class") <- "ham"
spam.2.corpus <- Corpus(VectorSource(all.spam.2))
meta(spam.2.corpus, tag = "class") <- "spam"
ham.2.corpus <- Corpus(VectorSource(all.ham.2))
meta(ham.2.corpus, tag = "class") <- "ham"
ham.corpus <- ham.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
spam.corpus <- spam.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
spam.2.corpus <- spam.2.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
ham.2.corpus <- ham.2.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
test.corpus <- Corpus(VectorSource(c(ham.corpus, spam.corpus)))
test.corpus <- sample(test.corpus)
train.corpus <- Corpus(VectorSource(c(ham.2.corpus, spam.2.corpus)))
train.corpus <- sample(train.corpus)
# spam.tdm <- TermDocumentMatrix(spam.corpus)
# spam.dtm <- DocumentTermMatrix(spam.corpus)
# spam.dtm <- removeSparseTerms(spam.dtm, 0.95)
#
# ham.tdm <- TermDocumentMatrix(ham.corpus)
# ham.dtm <- DocumentTermMatrix(ham.corpus)
# ham.dtm <- removeSparseTerms(ham.dtm, 0.95)
#
# spam.2.tdm <- TermDocumentMatrix(spam.2.corpus)
# spam.2.dtm <- DocumentTermMatrix(spam.2.corpus)
# spam.2.dtm <- removeSparseTerms(spam.2.dtm, 0.95)
#
# ham.2.tdm <- TermDocumentMatrix(ham.2.corpus)
# ham.2.dtm <- DocumentTermMatrix(ham.2.corpus)
# ham.2.dtm <- removeSparseTerms(ham.2.dtm, 0.95)
test.dtm <- DocumentTermMatrix(test.corpus)
test.dtm <- removeSparseTerms(test.dtm, 0.95)
train.dtm <- DocumentTermMatrix(train.corpus)
train.dtm <- removeSparseTerms(train.dtm, 0.95)
tot.dtm <- c(test.dtm, train.dtm)
length(tot.dtm)
length(all.labels)
all.labels <- c(rep("spam", length(list.files("spam/"))), rep("ham", length(list.files("easy_ham/"))), rep("spam", length(list.files("spam_2/"))), rep("ham", length(list.files("easy_ham_2/"))))
length(all.labels)
length(list.files("space/"))
count(list.files("space/"))
length(list.files("spam/"))
count(list.files("spam/"))
length(ham.one.dir)
length(ham.two.dir)
length(ham.one.dir+ham.two.dir)
length(ham.one.dir)+length(ham.two.dir)
length(spam.one.dir)+length(spam.two.dir)
3952+1002
length(spam.doc)
length(spam.docs)
length(spam.2.docs)
length(ham.docs)
length(ham.2.docs)
2551+2551+1000
length(all.labels)
length(ham.2.docs)
length(ham.one.dir)
length(list.files("spam/"))
length(list.files("spam/"))
length(all.labels)
length(list.files("easy_ham/"))
length(list.files("spam_2/"))
rep("ham", length(list.files("easy_ham_2/")))
length(list.files("easy_ham_2/"))
2551+2551
length(all.labels)
size(all.labels)
summary(all.labels)
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
all.labels <- c(rep("spam", length(list.files("spam/"))), rep("ham", length(list.files("easy_ham/"))), rep("spam", length(list.files("spam_2/"))), rep("ham", length(list.files("easy_ham_2/"))))
size(all.labels)
length(all.labels)
head(all.ham.2)
head(ham.2.docs)
length(ham.2.docs)
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
length(all.labels)
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
lb
length(tot.dtm)
tot.dtm <- c(DocumentTermMatrix(test.dtm), DocumentTermMatrix(train.dtm))
tot.dtm <- train.dtm + test.dtm
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
length(tot.dtm)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = t)
tot.dtm <- c(DocumentTermMatrix(test.corpus), DocumentTermMatrix(train.corpus))
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
length(tot.dtm)
length(all.labels)
1:lb
lb+1
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = ((lb+1):(length(all.labels))), virgin = F)
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
lb <- length(spam.docs) + length(ham.docs)
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
tot.dtm <- c(DocumentTermMatrix(test.corpus), DocumentTermMatrix(train.corpus))
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
head(sample(all.labels))
head(sample(all.labels))
View(test.dtm)
container <- create_container(test.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
container <- create_container(test.dtm+train.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
svm_model <- train_model(container, "SVM")
tot.dtm <- create_matrix(train.corpus, test.corpus)
tot.dtm <- create_matrix(train.corpus)
totl.dtm <- create_matrix(train.corpus)
totl.dtm <- create_matrix(train.corpus, test.corpus)
# tot.dtm <- c(DocumentTermMatrix(test.corpus), DocumentTermMatrix(train.corpus))
tot.dtm <- c(test.dtm, train.dtm)
test.dtm <- removeSparseTerms(test.dtm, 0.90)
train.dtm <- removeSparseTerms(train.dtm, 0.90)
# tot.dtm <- c(DocumentTermMatrix(test.corpus), DocumentTermMatrix(train.corpus))
tot.dtm <- c(test.dtm, train.dtm)
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(stringr)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
# Training Files
ham.one.dir <- list.files("easy_ham/", full.names = T, recursive = F)
spam.one.dir <- list.files("spam/", full.names = T, recursive = F)
# Testing Files
ham.two.dir <- list.files("easy_ham_2/", full.names = T, recursive = F)
spam.two.dir <- list.files("spam_2/", full.names = T, recursive = F)
# Combine testing files
comb <- c(ham.two.dir, spam.two.dir)
head(readLines(ham.one.dir[1]), 10)
tail(readLines(ham.one.dir[1]), 10)
get.msg <- function(path) {
con <- file(path, open="rt")
text <- readLines(con)
msg <- text[seq(which(text == "")[1] + 1, length(text))]
close(con)
return(paste(msg, collapse = "\n"))
}
spam.path <- "spam/"
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which( !str_detect(spam.docs,"^0000.") & spam.docs != 'cmds' )]
all.spam <- sapply(spam.docs, function(p)get.msg(file.path(spam.path, p, sep = "")))
ham.path <- "easy_ham/"
ham.docs <- dir(ham.path)
ham.docs <- ham.docs[which(ham.docs!="cmds")]
all.ham <- sapply(ham.docs, function(p)get.msg(paste(ham.path,p,sep="")))
spam.2.path <- "spam_2/"
spam.2.docs <- dir(spam.path)
spam.2.docs <- spam.docs[which( !str_detect(spam.docs,"^0000.") & spam.docs != 'cmds' )]
all.spam.2 <- sapply(spam.docs, function(p)get.msg(file.path(spam.path, p, sep = "")))
ham.2.path <- "easy_ham_2/"
ham.2.docs <- dir(ham.path)
ham.2.docs <- ham.docs[which(ham.docs!="cmds")]
all.ham.2 <- sapply(ham.docs, function(p)get.msg(paste(ham.path,p,sep="")))
spam.corpus <- Corpus(VectorSource(all.spam))
meta(spam.corpus, tag = "class") <- "spam"
ham.corpus <- Corpus(VectorSource(all.ham))
meta(ham.corpus, tag = "class") <- "ham"
spam.2.corpus <- Corpus(VectorSource(all.spam.2))
meta(spam.2.corpus, tag = "class") <- "spam"
ham.2.corpus <- Corpus(VectorSource(all.ham.2))
meta(ham.2.corpus, tag = "class") <- "ham"
ham.corpus <- ham.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
spam.corpus <- spam.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
spam.2.corpus <- spam.2.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
ham.2.corpus <- ham.2.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
test.corpus <- Corpus(VectorSource(c(ham.corpus, spam.corpus)))
test.corpus <- sample(test.corpus)
train.corpus <- Corpus(VectorSource(c(ham.2.corpus, spam.2.corpus)))
train.corpus <- sample(train.corpus)
# spam.tdm <- TermDocumentMatrix(spam.corpus)
# spam.dtm <- DocumentTermMatrix(spam.corpus)
# spam.dtm <- removeSparseTerms(spam.dtm, 0.95)
#
# ham.tdm <- TermDocumentMatrix(ham.corpus)
# ham.dtm <- DocumentTermMatrix(ham.corpus)
# ham.dtm <- removeSparseTerms(ham.dtm, 0.95)
#
# spam.2.tdm <- TermDocumentMatrix(spam.2.corpus)
# spam.2.dtm <- DocumentTermMatrix(spam.2.corpus)
# spam.2.dtm <- removeSparseTerms(spam.2.dtm, 0.95)
#
# ham.2.tdm <- TermDocumentMatrix(ham.2.corpus)
# ham.2.dtm <- DocumentTermMatrix(ham.2.corpus)
# ham.2.dtm <- removeSparseTerms(ham.2.dtm, 0.95)
test.dtm <- DocumentTermMatrix(test.corpus)
test.dtm <- removeSparseTerms(test.dtm, 0.90)
train.dtm <- DocumentTermMatrix(train.corpus)
train.dtm <- removeSparseTerms(train.dtm, 0.90)
# tot.dtm <- c(DocumentTermMatrix(test.corpus), DocumentTermMatrix(train.corpus))
tot.dtm <- c(test.dtm, train.dtm)
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(stringr)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
<<<<<<< HEAD
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
for (i in 1:length(comb)){
read_f <- readLines(comb[i])
read_f <- gsub(".*\n*", "", read_f)
}
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
setwd("~/Documents/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
# setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
setwd("~/Documents/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
=======
>>>>>>> 1be872a834670d7cf7e4e902e950577bdddf79aa
# Training Files
ham.one.dir <- list.files("easy_ham/", full.names = T, recursive = F)
spam.one.dir <- list.files("spam/", full.names = T, recursive = F)
# Testing Files
ham.two.dir <- list.files("easy_ham_2/", full.names = T, recursive = F)
spam.two.dir <- list.files("spam_2/", full.names = T, recursive = F)
# Combine testing files
comb <- c(ham.two.dir, spam.two.dir)
head(readLines(ham.one.dir[1]), 10)
tail(readLines(ham.one.dir[1]), 10)
<<<<<<< HEAD
for (i in 1:length(comb)){
read_f <- readLines(comb[i])
read_f <- gsub(".*\n*", "", read_f)
}
read_f
for (i in 1:length(comb)){
read_f <- readLines(comb[i])
comb[i] <- gsub(".*\n*", "", read_f)
}
readLines(comb[i])
knitr::opts_chunk$set(echo = TRUE, warn = F, message = F)
options(stringsAsFactors = F)
library(tidyverse)
library(wordcloud2)
library(tm)
library(quanteda)
library(RTextTools)
# setwd("~/GitHub/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
setwd("~/Documents/CUNY_MSDA/Fall_2017/DATA_607/DATA_607_Project_4")
# Training Files
ham.one.dir <- list.files("easy_ham/", full.names = T, recursive = F)
spam.one.dir <- list.files("spam/", full.names = T, recursive = F)
# Testing Files
ham.two.dir <- list.files("easy_ham_2/", full.names = T, recursive = F)
spam.two.dir <- list.files("spam_2/", full.names = T, recursive = F)
# Combine testing files
comb <- c(ham.two.dir, spam.two.dir)
head(readLines(ham.one.dir[1]), 10)
tail(readLines(ham.one.dir[1]), 10)
readLines(comb[i])
=======
get.msg <- function(path) {
con <- file(path, open="rt")
text <- readLines(con)
msg <- text[seq(which(text == "")[1] + 1, length(text))]
close(con)
return(paste(msg, collapse = "\n"))
}
spam.path <- "spam/"
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which( !str_detect(spam.docs,"^0000.") & spam.docs != 'cmds' )]
all.spam <- sapply(spam.docs, function(p)get.msg(file.path(spam.path, p, sep = "")))
ham.path <- "easy_ham/"
ham.docs <- dir(ham.path)
ham.docs <- ham.docs[which(ham.docs!="cmds")]
all.ham <- sapply(ham.docs, function(p)get.msg(paste(ham.path,p,sep="")))
spam.2.path <- "spam_2/"
spam.2.docs <- dir(spam.path)
spam.2.docs <- spam.docs[which( !str_detect(spam.docs,"^0000.") & spam.docs != 'cmds' )]
all.spam.2 <- sapply(spam.docs, function(p)get.msg(file.path(spam.path, p, sep = "")))
ham.2.path <- "easy_ham_2/"
ham.2.docs <- dir(ham.path)
ham.2.docs <- ham.docs[which(ham.docs!="cmds")]
all.ham.2 <- sapply(ham.docs, function(p)get.msg(paste(ham.path,p,sep="")))
spam.corpus <- Corpus(VectorSource(all.spam))
meta(spam.corpus, tag = "class") <- "spam"
ham.corpus <- Corpus(VectorSource(all.ham))
meta(ham.corpus, tag = "class") <- "ham"
spam.2.corpus <- Corpus(VectorSource(all.spam.2))
meta(spam.2.corpus, tag = "class") <- "spam"
ham.2.corpus <- Corpus(VectorSource(all.ham.2))
meta(ham.2.corpus, tag = "class") <- "ham"
ham.corpus <- ham.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
spam.corpus <- spam.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
spam.2.corpus <- spam.2.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
ham.2.corpus <- ham.2.corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, words = stopwords("english")) %>%
tm_map(removePunctuation) %>%
tm_map(stemDocument) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
test.corpus <- Corpus(VectorSource(c(ham.corpus, spam.corpus)))
test.corpus <- sample(test.corpus)
train.corpus <- Corpus(VectorSource(c(ham.2.corpus, spam.2.corpus)))
train.corpus <- sample(train.corpus)
tot.dtm <- c(DocumentTermMatrix(test.corpus), DocumentTermMatrix(train.corpus))
all.labels <- c(rep("spam", length(spam.docs)), rep("ham", length(ham.docs)), rep("spam", length(spam.2.docs)), rep("ham", length(ham.2.docs)))
lb <- length(spam.docs) + length(ham.docs)
container <- create_container(tot.dtm, labels = all.labels, trainSize = 1:lb, testSize = (lb+1):length(all.labels), virgin = F)
?debug
>>>>>>> 1be872a834670d7cf7e4e902e950577bdddf79aa
